{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendiendo la Tokenización y el Embedding con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se presenta un ejemplo sencillo de tokenización y embedding utilizando la biblioteca Keras. El ejemplo consiste en una lista de cuatro frases que serán sometidas a los procesos de tokenización y embedding. Luego, estos resultados serán utilizados como entrada para una red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de datos de entrada\n",
    "texts = ['Esto es un ejemplo de texto para clasificar.',\n",
    "         'Otro ejemplo de texto para probar el modelo.',\n",
    "         'Un tercer ejemplo para completar el conjunto de datos.',\n",
    "         'Último ejemplo para el ejemplo.']\n",
    "\n",
    "labels = np.array([0, 1, 0, 1])  # Ejemplo de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización y preparación de datos\n",
    "maxlen = 10  # Longitud máxima de secuencia\n",
    "training_samples = 3  # Número de muestras de entrenamiento\n",
    "validation_samples = len(texts) - training_samples  # Número de muestras de validación\n",
    "max_words = 10000  # Número máximo de palabras en el vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar es el proceso de **dividir un texto en partes más pequeñas** llamadas tokens. Estos tokens pueden ser **palabras individuales**, números, signos de puntuación o cualquier otra unidad significativa de texto.\n",
    "\n",
    "En el procesamiento del lenguaje natural (NLP), la tokenización es una tarea fundamental que se realiza antes de procesar el texto en modelos de aprendizaje automático. La tokenización ayuda a convertir el texto en una forma que pueda ser utilizada **como entrada para el modelo.**\n",
    "\n",
    "En resumen, tokenizar es el proceso de **dividir un texto** en partes más pequeñas (tokens) para su posterior procesamiento. Esto se hace generalmente para análisis de texto, como clasificación de texto, análisis de sentimientos, traducción automática, entre otros.\n",
    "\n",
    "El argumento num_words en Tokenizer(num_words=max_words) especifica **el número máximo de palabras** que se deben tener en cuenta al tokenizar el texto.\n",
    "\n",
    "Por ejemplo, si estableces num_words=10000, el tokenizador solo tendrá en cuenta **las 10,000 palabras más frecuentes** en el conjunto de datos y todas las demás palabras **se ignorarán**. Esto es útil para limitar el tamaño del vocabulario y, por lo tanto, el tamaño de los datos de entrada al modelo, lo que puede mejorar la eficiencia computacional y evitar problemas de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'ejemplo',\n",
       " 2: 'para',\n",
       " 3: 'de',\n",
       " 4: 'el',\n",
       " 5: 'un',\n",
       " 6: 'texto',\n",
       " 7: 'esto',\n",
       " 8: 'es',\n",
       " 9: 'clasificar',\n",
       " 10: 'otro',\n",
       " 11: 'probar',\n",
       " 12: 'modelo',\n",
       " 13: 'tercer',\n",
       " 14: 'completar',\n",
       " 15: 'conjunto',\n",
       " 16: 'datos',\n",
       " 17: 'último'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 17 tokens únicos.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Se encontraron %s tokens únicos.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer.texts_to_sequences(texts) **convierte una lista de textos en una lista de secuencias de números enteros**. Cada número entero **representa la posición** de la palabra en el vocabulario generado por el tokenizador. Cada lista dentro de la lista de salida representa una secuencia de números enteros correspondientes a las palabras en el texto original, de acuerdo con el índice asignado por el tokenizador.\n",
    "\n",
    "La codificación que se realiza con tokenizer.texts_to_sequences(texts) **no es una codificación one-hot**, sino que simplemente convierte cada palabra en una secuencia de números enteros de acuerdo con el índice de la palabra en el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 8, 5, 1, 3, 6, 2, 9],\n",
       " [10, 1, 3, 6, 2, 11, 4, 12],\n",
       " [5, 13, 1, 2, 14, 4, 15, 3, 16],\n",
       " [17, 1, 2, 4, 1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad_sequences(sequences, maxlen=maxlen) **ajusta la longitud** de todas las secuencias de números enteros **a una longitud máxima específica maxlen.**\n",
    "\n",
    "Si una secuencia tiene más de maxlen elementos, **se truncará** para que tenga una longitud de maxlen. Si una secuencia tiene menos de maxlen elementos, **se rellenará con ceros** al principio de la secuencia para que tenga una longitud de maxlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  7,  8,  5,  1,  3,  6,  2,  9],\n",
       "       [ 0,  0, 10,  1,  3,  6,  2, 11,  4, 12],\n",
       "       [ 0,  5, 13,  1,  2, 14,  4, 15,  3, 16],\n",
       "       [ 0,  0,  0,  0,  0, 17,  1,  2,  4,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen, padding=\"pre\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capa de Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa Embedding en Keras es una capa de procesamiento de texto que convierte números enteros positivos (índices de palabras) **en vectores de tamaño fijo**. **Cada palabra se representa por un vector** denso de valores reales (los parámetros del modelo son aprendidos automáticamente).\n",
    "\n",
    "Aquí está el significado de los argumentos de Embedding(max_words, 8, input_length=maxlen):\n",
    "\n",
    "- **input_dim:** El primer argumento **es el tamaño del vocabulario**, es decir, el número máximo **de palabras diferentes** que se pueden tener en cuenta. Las palabras fuera de este índice serán ignoradas.\n",
    "\n",
    "- **output_dim:** El segundo argumento es la dimensión del espacio vectorial en el que se incrustarán las palabras. En este caso, **cada palabra** se representará como un vector **de longitud 8**. El embedding se aplica **a nivel de palabras, no a nivel de frases completas**. Cuando se aplica la capa de embedding en Keras, **cada palabra** en la secuencia se convierte en un vector de longitud fija de acuerdo con la dimensión del embedding especificada. Estos vectores son luego procesados por las capas posteriores de la red neuronal. Por lo tanto, después de aplicar la capa de embedding, cada palabra en la secuencia de entrada **se representa como un vector**. La representación de la secuencia completa se obtiene **combinando estos vectores**, generalmente **promediándolos o concatenándolos**, dependiendo de la arquitectura de la red neuronal.\n",
    "\n",
    "- **input_length:** La longitud de las secuencias de entrada, que **debe ser la misma que la longitud de las secuencias después de aplicar pad_sequences**. En este caso, maxlen es la longitud máxima de las secuencias de entrada que se ajustó previamente utilizando pad_sequences. El argumento input_length en la capa Embedding **debe ser igual a la longitud de las secuencias después de aplicar el relleno (padding)**. Esto asegura que la capa Embedding **sepa qué longitud esperar para cada secuencia de entrada**. Entonces, si aplicaste pad_sequences con maxlen=maxlen, el argumento input_length de la capa Embedding debe ser igual a maxlen. Esto garantiza que las secuencias de entrada tengan la misma longitud que la que la capa Embedding espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n"
     ]
    }
   ],
   "source": [
    "# Definición del modelo\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=8, input_length=maxlen))\n",
    "\n",
    "#Obtener la salida de la capa de embedding\n",
    "embedded_data = model.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar la forma de la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de salida de la capa de embedding:  (4, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma de salida de la capa de embedding: \", embedded_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de la salida es (4, 10, 8), lo que significa que tenemos 4 secuencias en el lote, cada una con una longitud de 10 (debido al padding) y cada palabra representada por un vector de longitud 8 después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la capa de embedding:\n",
      "[[ 0.02984177 -0.02747539  0.03664869 -0.01651597 -0.03670455  0.04548267\n",
      "   0.00334885 -0.01498439]\n",
      " [ 0.02984177 -0.02747539  0.03664869 -0.01651597 -0.03670455  0.04548267\n",
      "   0.00334885 -0.01498439]\n",
      " [ 0.02323672 -0.04886472  0.02024076  0.03435335  0.00584662 -0.03813946\n",
      "  -0.02302487  0.04820934]\n",
      " [-0.02668855  0.02641291  0.01014973  0.03130524  0.04248685  0.04340789\n",
      "   0.04386213  0.02592975]\n",
      " [ 0.02296618  0.01268751 -0.00826931  0.02466289 -0.00603859  0.03556115\n",
      "  -0.04260237  0.02507292]\n",
      " [-0.03043361 -0.0152789   0.01945398  0.00390643 -0.00664817 -0.04933877\n",
      "   0.04003601  0.04925055]\n",
      " [ 0.0122527   0.00184907 -0.01474243 -0.00608959  0.03644953 -0.03419687\n",
      "  -0.00430704 -0.0497017 ]\n",
      " [ 0.04405561  0.02223634 -0.04763883 -0.01063512 -0.02597965 -0.0151841\n",
      "  -0.02315701 -0.03719201]\n",
      " [ 0.0383227   0.01189471 -0.00612334 -0.02129604 -0.0158491   0.04025948\n",
      "   0.0486761   0.00324627]\n",
      " [ 0.00995102  0.01222322  0.01195465 -0.03428517 -0.01158403  0.01882125\n",
      "   0.02029654 -0.00446489]]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar la salida de la capa de embedding\n",
    "print(\"Salida de la capa de embedding:\")\n",
    "print(embedded_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se continua con la construcción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se agregan más capas\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6949 - acc: 0.3333 - val_loss: 0.7025 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6907 - acc: 0.3333 - val_loss: 0.7030 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6865 - acc: 0.6667 - val_loss: 0.7035 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6823 - acc: 1.0000 - val_loss: 0.7040 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6782 - acc: 1.0000 - val_loss: 0.7045 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6740 - acc: 1.0000 - val_loss: 0.7050 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6699 - acc: 1.0000 - val_loss: 0.7054 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6658 - acc: 1.0000 - val_loss: 0.7059 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6617 - acc: 1.0000 - val_loss: 0.7063 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6576 - acc: 1.0000 - val_loss: 0.7067 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126127bf5b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "model.fit(data[:training_samples], labels[:training_samples],\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=(data[training_samples:], labels[training_samples:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6998 - acc: 0.0000e+00\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "loss, accuracy = model.evaluate(data[training_samples:], labels[training_samples:])\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
